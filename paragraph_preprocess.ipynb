{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "from contractions import CONTRACTION_MAP\n",
    "from nltk.corpus import wordnet\n",
    "from pattern3.en import suggest\n",
    "'''Note: need to catch error in empty try, line 37 of \n",
    "/Users/minhnguyen/.local/lib/python3.5/site-packages/pattern3/text/tree.py\n",
    "by replacing try-except with simply assignment izip=zip'''\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paragraph = \"\"\"The brown fox wasn't that quick and he couldn't win the race. \\\n",
    "Hey that's a great deal! I just bought a phone for $199. \\\n",
    "@@You'll (learn) a **lot** in the book. Python is an amazing language!@@\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence, keep_apostrophes=False):\n",
    "    '''Remove special characters from the sentence'''\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence\n",
    "\n",
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = list(filter(None, [pattern.sub('', token) for token in tokens]))\n",
    "    return filtered_tokens\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    '''Expand short form to full words ie. can't => cannot, isn't => is not'''\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    '''Remove not meaningful words, ie. the, is, a, an.\n",
    "    BE CAREFUL: this nltk.corpus.stopwords contains even meaningful words ie. not\n",
    "    so for now, donot touch it.'''\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    nochar_text = remove_characters_before_tokenization(text, keep_apostrophes=True)\n",
    "    text = expand_contractions(nochar_text, CONTRACTION_MAP)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    #word_tokens = [remove_stopwords(tokens) for tokens in word_tokens]\n",
    "    word_tokens = [remove_characters_after_tokenization(tokens) for tokens in word_tokens]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def suggest_spelling(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def remove_repeated(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return remove_repeated(new_word) if new_word != old_word else new_word\n",
    "    \n",
    "    def suggest_spell(old_word):\n",
    "        # if old_word makes sense, quit\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        \n",
    "        new_word = old_word\n",
    "        sugg = suggest(old_word)[0]\n",
    "        if sugg[1] > 0.4:  # catch any good suggestion\n",
    "            new_word = sugg[0]\n",
    "        else:  # if no good suggestion, try removing repeated chars and suggest again\n",
    "            sugg_1 = suggest(remove_repeated(old_word))[0]\n",
    "            if sugg_1[1] > 0.4:\n",
    "                new_word = sugg_1[0]\n",
    "        \n",
    "        if wordnet.synsets(new_word):\n",
    "            return new_word\n",
    "        return old_word\n",
    "    \n",
    "    correct_tokens = [suggest_spell(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "#suggest_spelling(['catt', 'finalllyy', 'carrs', 'carries'])\n",
    "\n",
    "#def lemmatize to get meaningful team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'brown',\n",
       "  'fox',\n",
       "  'was',\n",
       "  'not',\n",
       "  'that',\n",
       "  'quick',\n",
       "  'and',\n",
       "  'he',\n",
       "  'could',\n",
       "  'not',\n",
       "  'win',\n",
       "  'the',\n",
       "  'race'],\n",
       " ['Hey', 'that', 'is', 'a', 'great', 'deal'],\n",
       " ['I', 'just', 'bought', 'a', 'phone', 'for', '199'],\n",
       " ['You', 'will', 'learn', 'a', 'lot', 'in', 'the', 'book'],\n",
       " ['Python', 'is', 'an', 'amazing', 'language']]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
